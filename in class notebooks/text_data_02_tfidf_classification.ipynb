{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up, load data, and clean\n",
    "\n",
    "add language for each text excerpts, filter out non-English excerpts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/joshwinnes/Library/Mobile Documents/com~apple~CloudDocs/Wheaton College/fall 2024/topics in data science/data/\"\n",
    "sdg_names = pd.read_csv(data_dir + \"sdg_name_definition.csv\")\n",
    "\n",
    "text_file_name = \"osdg-community-data-v2024-04-01.csv\"\n",
    "text_df = pd.read_csv(data_dir + text_file_name,sep = \"\\t\",  quotechar='\"')\n",
    "text_df.drop(text_df.columns.values[0],axis = 1, inplace=True)\n",
    "text_df = text_df.query(\"agreement > 0.5 and (labels_positive - labels_negative) > 2\").reset_index(drop=True)\n",
    "text_df[\"lang\"] = text_df[\"text\"].apply(lambda x: detect(x))\n",
    "text_df = text_df.query(\"lang == 'en'\")\n",
    "text_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sdg                                                     1\n",
       "sdg_name                                       No Poverty\n",
       "sdg_definition    End poverty in all its forms everywhere\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdg_names.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26472, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of documents using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = text_df.text\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer.fit(corpus)\n",
    "count_vector = count_vectorizer.transform(corpus).toarray() \n",
    "count_vector_df = pd.DataFrame(count_vector, columns=count_vectorizer.get_feature_names_out())\n",
    "term_freq = pd.DataFrame({\"term\": count_vector_df.columns.values, \"freq\" : count_vector_df.sum(axis=0)})\n",
    "term_freq.sort_values(by=\"freq\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at language models and feature for the task of document classification\n",
    " - split the data into train and test\n",
    " - construct text feature vectors using \n",
    "  - CountVectorizer, TfidfVectorizer\n",
    "  - with unigram, bigram, and unigram and bigram combined\n",
    " - use Naive Bayes - multinomialNB implementation to assess the feature vectors' effectiveness\n",
    " - inspect the top informative features (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = text_df.text\n",
    "sdg_num = text_df.sdg\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(corpus, sdg_num, test_size=0.33, random_state=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using count vectors as feature and use multinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features = bigrams (ngram_range = (2,2))\n",
    "\n",
    "remove stop words (stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count_vectorizer = CountVectorizer(ngram_range=(2,2), stop_words = \"english\" )\n",
    "X_train_count_vectorizer.fit(X_train) \n",
    "X_train_count_vector = X_train_count_vectorizer.transform(X_train) \n",
    "X_test_count_vector = X_train_count_vectorizer.transform(X_test) \n",
    "\n",
    "count_multinomialNB_clf = MultinomialNB().fit(X_train_count_vector, y_train)\n",
    "y_pred = count_multinomialNB_clf.predict(X_test_count_vector)\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "font = {'family': 'sans-serif', 'weight': 'heavy','size': 7,}\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, text_kw=font, ax=ax, cmap=mpl.colormaps[\"YlGnBu\"],colorbar=None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - understanding metrics in the context of multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually check precision for sdg 1\n",
    "tp = 389\n",
    "fp = 113 #(summing vertically below 389)\n",
    "fn = 96 #(summing horizentally to the right of 398)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2 * (precision * recall) /(precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('precision for SDG 1: {:.4}'.format(precision))\n",
    "print('recall for SDG 1: {:.4}'.format(recall))\n",
    "print('f1 for SDG 1: {:.4}'.format(f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - inspecting (and comparing) performance through classification_report\n",
    " - looking at row #1 for SDG 1, it should match with above numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,y_pred, digits = 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - inspect individual scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy = {:.4}\".format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print(\"macro-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'macro')))\n",
    "print(\"micro-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'micro')))\n",
    "print(\"macro-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'macro')))\n",
    "print(\"micro-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'micro')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - put it all together\n",
    " - unigram and bigram together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count_vectorizer = CountVectorizer(ngram_range=(1,2), stop_words = \"english\" )\n",
    "X_train_count_vectorizer.fit(X_train)  \n",
    "X_train_count_vector = X_train_count_vectorizer.transform(X_train) \n",
    "X_test_count_vector = X_train_count_vectorizer.transform(X_test) \n",
    "\n",
    "count_multinomialNB_clf = MultinomialNB().fit(X_train_count_vector, y_train)\n",
    "y_pred = count_multinomialNB_clf.predict(X_test_count_vector)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, text_kw=font, ax=ax, cmap=mpl.colormaps[\"YlGnBu\"])\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(\"accuracy = {:.4}\".format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print(\"macro-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'macro')))\n",
    "print(\"macro-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'macro')))\n",
    "print(\"weighted-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'weighted')))\n",
    "print(\"weighted-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'weighted')))\n",
    "print(\"micro-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'micro')))\n",
    "print(\"micro-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'micro')))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above, it looks like using both unigram and bigram performed better than using only bigram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf vector with multinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words = \"english\" )\n",
    "X_train_tfidf_vectorizer.fit(X_train)\n",
    "X_train_tfidf_vector = X_train_tfidf_vectorizer.transform(X_train) \n",
    "X_test_tfidf_vector = X_train_tfidf_vectorizer.transform(X_test) \n",
    "\n",
    "tfidf_multinomialNB_clf = MultinomialNB().fit(X_train_tfidf_vector, y_train)\n",
    "y_pred = tfidf_multinomialNB_clf.predict(X_test_tfidf_vector)\n",
    "\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(\"accuracy = {:.4}\".format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print(\"macro-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'macro')))\n",
    "print(\"macro-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'macro')))\n",
    "print(\"weighted-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'weighted')))\n",
    "print(\"weighted-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'weighted')))\n",
    "print(\"micro-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'micro')))\n",
    "print(\"micro-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'micro')))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, text_kw=font, ax=ax,cmap=mpl.colormaps[\"YlGnBu\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using multinomialNB on tfidf vectors seems to perform worse\n",
    "\n",
    " - let's run one more using tfidf but with bigram only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words = \"english\" )\n",
    "X_train_tfidf_vectorizer.fit(X_train)\n",
    "X_train_tfidf_vector = X_train_tfidf_vectorizer.transform(X_train) \n",
    "X_test_tfidf_vector = X_train_tfidf_vectorizer.transform(X_test) \n",
    "\n",
    "tfidf_multinomialNB_clf = MultinomialNB().fit(X_train_tfidf_vector, y_train)\n",
    "y_pred = tfidf_multinomialNB_clf.predict(X_test_tfidf_vector)\n",
    "\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(\"accuracy = {:.4}\".format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print(\"macro-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'macro')))\n",
    "print(\"macro-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'macro')))\n",
    "print(\"weighted-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'weighted')))\n",
    "print(\"weighted-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'weighted')))\n",
    "print(\"micro-averaged precision = {:.4}\".format(metrics.precision_score(y_test, y_pred, average = 'micro')))\n",
    "print(\"micro-averaged recall = {:.4}\".format(metrics.recall_score(y_test, y_pred, average = 'micro')))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, text_kw=font, ax=ax,cmap=mpl.colormaps[\"YlGnBu\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multinomialNB on ifidf vector performed the same on unigram + bigram, or bigram only\n",
    " - \n",
    " Re-run the above cells to compare performances with different parameters\n",
    "  - multinomialNB on count vectors of bigram only\n",
    "  - multinomialNB on count vectors of unigram and bigram\n",
    "  - multinomialNB on tfidf vectors of unigram and bigram\n",
    "  - multinomialNB on tfidf vectors of bigram only\n",
    "\n",
    "any conclusion is specific to this corpus, not to be generalized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at the most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words = \"english\" )\n",
    "X_train_tfidf_vectorizer.fit(X_train)\n",
    "labels = X_train_tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "X_train_tfidf_vector = X_train_tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf_vector = X_train_tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "tfidf_multinomialNB_clf = MultinomialNB().fit(X_train_tfidf_vector, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* feature_log_prob_ is ndarray of shape (n_classes, n_features), producing the empirical log probability of features given a class, P(x_i | y)\n",
    "* tfidf_multinomialNB_clf.classes_ produces the class labels known to the classifier, tfidf_multinomialNB_clf.classes_[0] is 1, meaning SDG 1.\n",
    "so we know the arrangement of the results in the order of sdg_num. \n",
    "* tfidf_multinomialNB_clf.feature_log_prob_[0] gives the empirical probabilities (log) of each feature given class SDG 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_multinomialNB_clf.classes_[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_multinomialNB_clf.feature_log_prob_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n",
    "    for labelid in classlabel:\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        top_n = sorted(zip(classifier.feature_log_prob_[labelid], feature_names), reverse=True)[:n]\n",
    "        for coef, feat in top_n:\n",
    "            print(\"SDG {} : {:30}  {:.6}\".format(labelid+1, feat, coef))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_informative_feature_for_class(X_train_tfidf_vectorizer,tfidf_multinomialNB_clf, [0, 13], n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count_vectorizer = CountVectorizer(ngram_range=(2,2), stop_words = \"english\" )\n",
    "X_train_count_vectorizer.fit(X_train)  \n",
    "X_train_count_vector = X_train_count_vectorizer.transform(X_train) \n",
    "X_test_count_vector = X_train_count_vectorizer.transform(X_test) \n",
    "\n",
    "count_multinomialNB_clf = MultinomialNB().fit(X_train_count_vector, y_train)\n",
    "most_informative_feature_for_class(X_train_count_vectorizer,count_multinomialNB_clf, [0, 13], n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_informative_feature_for_class(X_train_count_vectorizer,count_multinomialNB_clf, [8], n = 20)\n",
    "most_informative_feature_for_class(X_train_tfidf_vectorizer,tfidf_multinomialNB_clf, [8], n = 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observation on computation cost on toarray or not\n",
    " - when doing\n",
    "    X_train_tfidf_vector = X_train_tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "    X_test_tfidf_vector = X_train_tfidf_vectorizer.transform(X_test)\n",
    "\n",
    " - vs. doing\n",
    "\n",
    "    X_train_tfidf_vector = X_train_tfidf_vectorizer.transform(X_train).toaray()\n",
    "\n",
    "    X_test_tfidf_vector = X_train_tfidf_vectorizer.transform(X_test).toarray()\n",
    "    \n",
    " - adding .toarray() make the model fitting and prediction much slower\n",
    " - the former (without toarray()) is about 10 second, on (2, 2) tfidf\n",
    " - the later is about 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_n_features(vectorizer, clf, class_labels, n=10):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top_n = np.argsort(clf.feature_log_prob_[i])[::-1][:n]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" || \".join(feature_names[j] for j in top_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_n_features(X_train_tfidf_vectorizer,tfidf_multinomialNB_clf,[0,1,2,3,4,5,6,7,8,9,10],n=20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# try todense() and compare with toarray(), no speed gain observed\n",
    "corpus = text_df.text\n",
    "count_vectorizer = CountVectorizer()  # add stop_words = 'english' to remove stop words\n",
    "count_vector_dense = count_vectorizer.fit_transform(corpus).todense() # produce a dense representation for mamory order, possible for performance gain, but have not observed much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7015c692052df65c0c3bfe69601ef4de1426eb8c633bc2483af98a651ee94838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
